{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32cc258",
   "metadata": {},
   "source": [
    "# Custom PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6724fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "OS:  Windows-11-10.0.26100-SP0\n",
      "Python Version:  3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites:\n",
    "import sys, os, platform \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import product\n",
    "from glob import glob\n",
    "\n",
    "import pickle\n",
    "import open3d as o3d\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"OS: \", platform.platform())\n",
    "print(\"Python Version: \", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ee609",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7abe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, features_used, has_ground_truth=True, is_training=True, num_points=4096, label_map_file=\"label_mapping.pkl\", compute_normals=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        Args:\n",
    "            files (list): List of file paths to load data from.\n",
    "            features_used (list): List of features to extract (e.g., [\"xyz\", \"rgb\", \"i\"]).\n",
    "            has_ground_truth (bool): Whether the data has ground truth labels.\n",
    "            is_training (bool): Whether the dataset is for training or testing.\n",
    "            num_point (int): The number of points per tile.\n",
    "            label_map_file (str): Path to save/load the label mapping file.\n",
    "            compute_normals (bool): Whether to compute normals for the point cloud. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data, self.labels = self.load_data_and_labels(files,features_used, has_ground_truth, num_points, compute_normals)\n",
    "        \n",
    "        self.inputs = [self.preprocess(cloud_data)for cloud_data in self.data]\n",
    "\n",
    "        if has_ground_truth:\n",
    "            if is_training:\n",
    "                all_labels = torch.cat(self.labels)\n",
    "                unique_labels = np.unique(all_labels)\n",
    "                self.label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "                with open(label_map_file,\"wb\")as f:\n",
    "                    pickle.dump(self.label_mapping,f)\n",
    "\n",
    "            else: # Not training, load the label mapping from file\n",
    "                with open(label_map_file,\"rb\")as f:\n",
    "                    self.label_mapping = pickle.load(f)\n",
    "            self.num_classes = len(self.label_mapping.keys())\n",
    "            for i in range(0,len(self.labels)):\n",
    "                self.labels[i] = torch.tensor([self.label_mapping[label.item()]for label in self.labels[i]])\n",
    "        \n",
    "        else: # No ground truth labels\n",
    "            self.labels=[]\n",
    "            for i in range(0,len(self.inputs)):\n",
    "                self.labels.append(torch.empty_like(self.inputs[0]))\n",
    "            self.num_classes=0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of items in the dataset.\n",
    "        Returns:\n",
    "            int: Number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.inputs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input data, labels, and index.\n",
    "    \"\"\"\n",
    "        return self.inputs[index],self.labels[index],index\n",
    "    \n",
    "\n",
    "    def get_data(self, index):\n",
    "        \"\"\"\n",
    "        Get the input data at the specified index.\n",
    "        Args:\n",
    "            index (int): Index of the item to retrieve.\n",
    "        Returns:\n",
    "            torch.Tensor: The input data at the specified index.\n",
    "        \"\"\"\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "    def cloud_loader(self, pcd_name, features_used, compute_normals=False):\n",
    "        \"\"\"\n",
    "        Load point cloud data from a file and extract features based on the provided feature list.\n",
    "        Args:\n",
    "            pcd_name (str): Path to the point cloud file.\n",
    "            features_used (list): List of features to extract (e.g., [\"xyz\", \"rgb\", \"i\"]).\n",
    "            compute_normals (bool): Whether to compute normals for the point cloud. Default is False.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the features and ground truth labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load point cloud data from a file\n",
    "        cloud_data = np.loadtxt(pcd_name).transpose()\n",
    "        \n",
    "        # Extract features based on the provided feature list\n",
    "        features = []\n",
    "        if \"xyz\" in features_used:\n",
    "            n_coords = cloud_data[:3]\n",
    "            features.append(n_coords)\n",
    "        \n",
    "        if \"rgb\" in features_used:\n",
    "            colors = cloud_data[3:6] / 255.0\n",
    "            features.append(colors)\n",
    "\n",
    "        if \"i\" in features_used:\n",
    "            IRQ = np.quantile(cloud_data[-2], 0.75) - np.quantile(cloud_data[-2], 0.25)\n",
    "            n_intensity = ((cloud_data[-2] - np.median(cloud_data[-2])) / IRQ)\n",
    "            features.append(n_intensity)\n",
    "\n",
    "        if compute_normals:\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(cloud_data[:3].transpose())\n",
    "            pcd.estimate_normals()\n",
    "            normals = np.asarray(pcd.normals).transpose()\n",
    "            features.append(normals)\n",
    "\n",
    "        # Get ground truth labels  \n",
    "        gt = cloud_data[-1]\n",
    "        \n",
    "        return np.vstack(features), gt\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def tile_point_cloud(self, data, labels, num_points):\n",
    "        \"\"\"\n",
    "        Tile the point cloud data into smaller sections based on the specified number of points per tile.\n",
    "        Args:\n",
    "            data (numpy.ndarray): The point cloud data (shape: [3, N]).\n",
    "            labels (numpy.ndarray): The ground truth labels (shape: [N, ]).\n",
    "            num_points (int): The number of points per tile.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the tiled point cloud data and their corresponding labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        tree = KDTree(data[:2,:].T)\n",
    "        total_points = data.shape[1]\n",
    "        total_area = (data[0,:].max()-data[0,:].min())*(data[1,:].max()-data[1,:].min())\n",
    "        tile_area = total_area * num_points / total_points\n",
    "        tile_size = np.sqrt(tile_area)\n",
    "\n",
    "        # x, y ranges\n",
    "        x_min, y_min = data[:2,:].min(axis=1)\n",
    "        x_max, y_max = data[:2,:].max(axis=1)\n",
    "\n",
    "        # x, y coordinates ranges\n",
    "        x_coords = np.arange(x_min, x_max, tile_size)\n",
    "        y_coords = np.arange(y_min, y_max, tile_size)\n",
    "\n",
    "        tiles = []\n",
    "        tile_labels = []\n",
    "\n",
    "        for x,y in product(x_coords, y_coords):\n",
    "            tile_points_indices = tree.query_ball_point([x,y], tile_size)\n",
    "            if tile_points_indices:\n",
    "                n_indeces = len(tile_points_indices)\n",
    "                if n_indeces != num_points:\n",
    "                    # Randomly sample points if the number of points is not equal to num_points\n",
    "                    replace = n_indeces < num_points\n",
    "                    tile_points_indices = np.random.choice(tile_points_indices, num_points, replace=replace)\n",
    "                # Get the points and labels for the tile\n",
    "                tile_points = data[:, tile_points_indices]\n",
    "                tiles.append(tile_points)\n",
    "                tile_labels.append(labels[tile_points_indices])\n",
    "                tile_label=labels[tile_points_indices]if labels is not None else np.zeros(num_points)\n",
    "                tile_labels.append(tile_label)\n",
    "\n",
    "        return tiles, tile_labels\n",
    "\n",
    "\n",
    "\n",
    "    def load_data_and_labels(self, files, features_used, has_ground_truth, num_points, compute_normals=False):\n",
    "        \"\"\"\n",
    "        Load data and labels from the specified files.\n",
    "        Args:\n",
    "            files (list): List of file paths to load data from.\n",
    "            features_used (list): List of features to extract (e.g., [\"xyz\", \"rgb\", \"i\"]).\n",
    "            has_ground_truth (bool): Whether the data has ground truth labels.\n",
    "            num_point (int): The number of points per tile.\n",
    "            compute_normals (bool): Whether to compute normals for the point cloud. Default is False.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the preprocessed data and labels.\n",
    "        \"\"\"\n",
    "        data_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for file in files:\n",
    "            cloud_data, gt = self.cloud_loader(file, features_used, compute_normals)\n",
    "            tiles, tile_labels = self.tile_point_cloud(cloud_data, gt if has_ground_truth else None, num_points)\n",
    "\n",
    "            tiles = [torch.tensor(tile).float() for tile in tiles]\n",
    "            tile_labels = [torch.tensor(label).float() for label in tile_labels]    \n",
    "\n",
    "            data_list.extend(tiles)\n",
    "            labels_list.extend(tile_labels)\n",
    "\n",
    "        return data_list, labels_list\n",
    "    \n",
    "\n",
    "    def preprocess(self, cloud_data):\n",
    "        \"\"\"\n",
    "        Preprocess the point cloud data by centering it around the origin.\n",
    "        Args:\n",
    "            cloud_data (torch.Tensor): The point cloud data (shape: [3, N]).\n",
    "        Returns:\n",
    "            torch.Tensor: The preprocessed point cloud data.\n",
    "        \"\"\"\n",
    "        cloud_data = cloud_data.clone()\n",
    "        min_f = torch.min(cloud_data,dim=1).values\n",
    "        mean_f = torch.mean(cloud_data,dim=1)\n",
    "        correction = torch.tensor([mean_f[0],mean_f[1],min_f[2]])[:,None]\n",
    "        cloud_data[0:3] -= correction\n",
    "\n",
    "        return cloud_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d9821",
   "metadata": {},
   "source": [
    "### Create Training, Validation, and Testing point cloud file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9895fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of training and testing files\n",
    "project_dir = 'data/aerial_01'\n",
    "pc_train_files = glob(os.path.join(project_dir,\"train/*.txt\"))\n",
    "pc_test_files = glob(os.path.join(project_dir,\"test/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4680f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Validation file set from training set\n",
    "val_index = np.random.choice(len(pc_train_files), int(len(pc_train_files)/5), replace=False)\n",
    "val_list = [pc_train_files[i]for i in val_index]\n",
    "train_list = [pc_train_files[i] for i in np.setdiff1d(list(range(len(pc_train_files))), val_index)]\n",
    "test_list = pc_test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18ba531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training files: 101\n",
      "# of validation files: 25\n",
      "# of test files: 17\n"
     ]
    }
   ],
   "source": [
    "# Check sizes of datasets\n",
    "print(f\"# of training files: {len(train_list)}\")\n",
    "print(f\"# of validation files: {len(val_list)}\")\n",
    "print(f\"# of test files: {len(test_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54106d",
   "metadata": {},
   "source": [
    "### Use CustomDataset class to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92ef7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_points = 4096\n",
    "train_dataset = CustomDataset(train_list, \"xyz\", num_points=nr_points)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = CustomDataset(val_list, \"xyz\", is_training=False, num_points=nr_points)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = CustomDataset(test_list, \"xyz\", is_training=False, num_points=nr_points)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0e34ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  5\n"
     ]
    }
   ],
   "source": [
    "# Check number of classes\n",
    "num_classes = train_dataset.num_classes\n",
    "print(\"Number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c239529",
   "metadata": {},
   "source": [
    "### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddfec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Visualize the point clouds in the dataset using Open3D.\n",
    "    Args:\n",
    "        dataset (CustomDataset): The dataset containing point clouds.\n",
    "    \"\"\"\n",
    "    pcds = []\n",
    "    nb_pcd = len(dataset)\n",
    "    for i in range(nb_pcd):\n",
    "        point_cloud = dataset.get_data(i)\n",
    "        if isinstance(point_cloud,torch.Tensor):\n",
    "            point_cloud = point_cloud.cpu().numpy()\n",
    "        point_cloud = point_cloud[:3].T\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "        pcd.estimate_normals()\n",
    "\n",
    "        random_color = np.random.random((3,))\n",
    "        n = len(pcd.points)\n",
    "        colors = np.tile(random_color,(n,1))\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        pcds.append(pcd)\n",
    "        \n",
    "    o3d.visualization.draw_geometries(pcds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65626075",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db8072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6144b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
